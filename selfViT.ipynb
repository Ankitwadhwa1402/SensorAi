{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39da3317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e265a7b7",
   "metadata": {},
   "source": [
    "# Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21234a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Split image into patches and then embed them.\\nParameters\\nimg_size in\\n       Size of the image(it isasquare).\\npatch_size:int\\n       Size of the patch(it isasquare).\\nin_chans:int\\n       Number of input channels.\\nembed_dim: int\\n       The emmbedding dimension.\\nAttributes\\nn_patches int\\n    Number of patches inside of our image.\\nproj:nn.Conv2d\\n    Convolutional layer that does both the splitting into patches\\n    and their embedding.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Split image into patches and then embed them.\n",
    "Parameters\n",
    "img_size in\n",
    "       Size of the image(it isasquare).\n",
    "patch_size:int\n",
    "       Size of the patch(it isasquare).\n",
    "in_chans:int\n",
    "       Number of input channels.\n",
    "embed_dim: int\n",
    "       The emmbedding dimension.\n",
    "Attributes\n",
    "n_patches int\n",
    "    Number of patches inside of our image.\n",
    "proj:nn.Conv2d\n",
    "    Convolutional layer that does both the splitting into patches\n",
    "    and their embedding.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea1fc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.module.Module"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Module\n",
    "# PyTorch nn module has high-level APIs to build a neural network. \n",
    "# Torch.nn module uses Tensors and\n",
    "# Automatic differentiation modules for training \n",
    "# and building layers such as input, hidden, and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51fb3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self,img_size, patch_size, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size=img_size\n",
    "        self.patch_size=patch_size\n",
    "        self.n_patches=(img_size // patch_size)**2\n",
    "        self.proj=nn.Conv2d(\n",
    "                    in_chans,\n",
    "                    embed_dim,\n",
    "                    kernel_size=patch_size,\n",
    "                    stride=patch_size,\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"Run forward pass.\n",
    "            \n",
    "            Parameters\n",
    "            x: torch.Tensor\n",
    "                  Shape `(n_samples,in_chans,img_size,img_size)`.\n",
    "            \n",
    "            Returns\n",
    "            \n",
    "            torch.Tensor\n",
    "                  Shape `(n_samples,n_patches,embed_dim)`.\n",
    "            \"\"\"\n",
    "        x= self.proj(\n",
    "                    x\n",
    "                ) #(n_samples,embed_dim,n_patches**0.5,n.patches** 0.5)\n",
    "        x=x.flatten(2)  #(n_samples,embed_dim,n_patches**0.5)\n",
    "        x=x.transpose(1,2) # (n_samples,n_patches,embed_dim)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c2cbb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention mechanism.\n",
    "        Parameters\n",
    "            dim:in\n",
    "                    The input and out dimension of per token features.\n",
    "            n_heads:int\n",
    "                    Number of attention heads.\n",
    "            qkv_bias:bool\n",
    "                    If True then we include bias to the query,key and value projecti\n",
    "            attn_p:float\n",
    "                    Dropout probability applied to the query,key and value tensors.\n",
    "            proj_p: float\n",
    "                    Dropout probability applied to the output tensor.\"\"\"\n",
    "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0, proj_p=0):\n",
    "        super().__init__()\n",
    "        self.n_heads=n_heads\n",
    "        self.dim=dim\n",
    "        self.head_dim=dim//n_heads  # we setup in that way that once we concatenate all the attention heads\n",
    "                                    # we will get a new tensor that will have same dimensionality as the input\n",
    "\n",
    "                \n",
    "        \"\"\"Attributes\n",
    "            scale: float\n",
    "               Normalizing consant for the dot product.\n",
    "            qkv : nn.Linear\n",
    "                Linear projection for the query,key and value.\n",
    "            proj: nn.Linear\n",
    "                Linear mapping that takes in the concatenated output of all attention\n",
    "                heads and maps it intoanew space.\n",
    "            attn_drop,proj_drop: nn.Dropout\n",
    "                Dropout layers.\"\"\"\n",
    "        self.scale=self.head_dim**-0.5  \n",
    "        # scale idea is coming from \"Attention All you need \"\n",
    "        # the idea behind it is not to feed extremely large values into the softmax which could\n",
    "        # lead to small gradients\n",
    "        self.qkv=nn.Linear(dim, dim*3, bias=qkv_bias)\n",
    "        self.attn_drop=nn.Dropout(attn_p)\n",
    "        self.proj=nn.Linear(dim,dim)\n",
    "        self.proj_drop=nn.Dropout(proj_p)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        \"\"\"Run forward pass.\n",
    "            Parameters\n",
    "            xtorch.Tensor\n",
    "                   Shape(n_samples,n_patches+1,dim)`.\n",
    "            Returns\n",
    "            torch.Tensor\n",
    "                   Shape(n_samples,n_patches+1,dim)`.\"\"\"\n",
    "        n_samples,n_tokens,dim=x.shape\n",
    "\n",
    "        if(dim!=self.dim):\n",
    "            raise ValueError\n",
    "\n",
    "        qkv=self.qkv(x) #(n_samples, _patches, 3*dim)\n",
    "        qkv=qkv.reshape(\n",
    "            n_samples, n_tokens, 3, self.n_heads, self.head_dim\n",
    "            ) # (n_samples, n_patches+1, 3, n_heads, head_dim)\n",
    "        qkv=qkvpermute(\n",
    "                2,0,3,1,4\n",
    "            ) # (3, n_samples, n_heads, n_patches+1, head_dim)\n",
    "        q=qkv[0], k=qkv[1], v=qkv[2]\n",
    "        k_t=k.transpose(-2,-1) #(n_samples, n_head, head_dim, n_patches+1)\n",
    "        #for dot product\n",
    "\n",
    "        dp=(\n",
    "            q @ k_t\n",
    "        )* self.scale #(n_samples, n_heads,n_patches+1, n_patches+1)\n",
    "        attn=dp.softmax(dim=-1) #(n_samples,n_head, n_patches+1, head_dim)\n",
    "        attn=self.attn_drop(attn)\n",
    "        \n",
    "        weighted_avg=attn @ v # (n_samples, n_heads, n_patches+1, head_dim)\n",
    "        weighted_avg=weighted_avg.transpose(\n",
    "                            1,2\n",
    "                            ) #(n_samples, n_patches+1, n_heads, head_dim) \n",
    "         \n",
    "        weigthed_avg= wieghted_avg.flatten(2) #(n_samples,n_patches+1,dim)\n",
    "        \n",
    "        x=self.proj(weighted_avg) #(n_samples,n_patches+1, dim)\n",
    "        x=self.proj_drop(x) #(n_samples, n_patches+1, dim)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65207072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):   #Multi Layer Perception\n",
    "    \"\"\"Multilayer perceptron.\n",
    "        Parameters\n",
    "        in_features:int\n",
    "                Number of input features.\n",
    "                \n",
    "        hidden_features:int\n",
    "                Number of nodes in the hidden layer.\n",
    "                \n",
    "        out_features:int\n",
    "                Number of output features.\n",
    "                \n",
    "        p:float\n",
    "                Dropout probability.\"\"\"\n",
    "    def __init__ (self, in_features, hidden_features, out_features, p=0.):\n",
    "        super().__init__()           \n",
    "        \n",
    "        \"\"\"Attributes\n",
    "        fc: nn.Linear\n",
    "            The First linear layer.\n",
    "            \n",
    "        act:nn.GELU    (Gaussian error linear Unit Activation function)\n",
    "            GELU activation function.\n",
    "            \n",
    "        fc2: nn.Linear\n",
    "            The second linear layer.\n",
    "            \n",
    "        drop nn.Dropout\n",
    "            Dropout layer.\"\"\"\n",
    "        self.fc1=nn.Linear(in_features,hidden_features)\n",
    "        self.act=nn.GELU()\n",
    "        self.fc2=nn.Linear(hidden_features,out_features)\n",
    "        self.drop=nn.Dropout(p)\n",
    "    def forward(self,x):\n",
    "            \"\"\"Run forward pass.\n",
    "                Parameters\n",
    "                x: torch.Tensor\n",
    "                       Shape(n_samples,n_patches+1,in_features)`.\n",
    "                       \n",
    "                Returns\n",
    "                torch.Tensor\n",
    "                       Shape(n_samples,n_patches +1,out_features)`\"\"\"\n",
    "            x=self.fc1(\n",
    "                        x\n",
    "                    ) # (n_samples, n_patches+1, hidden_features)\n",
    "            x=self.act(x) # (n_samples, n_patches+1, hidden_features)\n",
    "            x=self.drop(x) # (n_samples, n_patches+1, hidden_features)\n",
    "            x=self.fc2(x) # (n_samples, n_patches+1, hidden_features)\n",
    "            x=self.drop(x) # (n_samples, n_patches+1, hidden_features)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b5ff42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block.\n",
    "        Parameters\n",
    "        dim:int\n",
    "                Embeddinig dimension.\n",
    "        n_heads: int\n",
    "                Number of attention heads.\n",
    "        mlp_ratio:float\n",
    "                Determines the hidden dimension size of the MLP module with respect\n",
    "                to'dim.\n",
    "        qkv_bias:bool\n",
    "                If True then we include bias to the query,key and value projections.\n",
    "        p,attn_p:float\n",
    "                Dropout probability.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.):\n",
    "        super().__init__()        \n",
    "        \"\"\"Attributes\n",
    "        norm1,norm2:LayerNorm\n",
    "                Layer normalization.\n",
    "                Attributes\n",
    "        norm1,norm2:LayerNorm\n",
    "            Layer normalization.\n",
    "        attn:Attention\n",
    "            Attention module.\n",
    "        mlp:MLP\n",
    "            MLP module.\"\"\"\n",
    "        self.norm1=nn.LayerNorm(dim,eps=1e-6)\n",
    "        self.attn=Attention(\n",
    "                    dim,\n",
    "                    n_heads=n_heads,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    attn_p=attn_p,\n",
    "                    proj_p=p\n",
    "            )\n",
    "        \n",
    "        self.norm2=nn.LayerNorm(dim,eps=1e-6)\n",
    "        hidden_features=int(dim*mlp_ratio)\n",
    "        self.mlp=MLP(\n",
    "                    in_features=dim,\n",
    "                    hidden_features=hidden_features,\n",
    "                    out_features=dim\n",
    "                    )\n",
    "    def forward(self,x):\n",
    "        \"\"\"Run forward pass.\n",
    "            Parameters\n",
    "            x:torch.Tensor\n",
    "                   Shape(n_samples,n_patches+1,dim)`.\n",
    "            Returns\n",
    "            torch.Tensor\n",
    "                   Shape(n_samples,n_patches+1,dim)`.\"\"\"\n",
    "        x=x+self.attn(self.norm1(x))\n",
    "        x=x+self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd7ea711",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Simplified implementation of the Vision transformer.\n",
    "        Parameters\n",
    "        img_size:int\n",
    "                Both height and the width of the image(it isasquare).\n",
    "        patch_size:int\n",
    "                Both height and the width of the patch(it isasquare).\n",
    "        in_chans: int\n",
    "                Number of input channels.\n",
    "        n_classes: int\n",
    "                Number of classes.\n",
    "        embed_dim: int\n",
    "                Dimensionality of the token/patch embeddings.\n",
    "        depth: int\n",
    "                Number of blocks.\n",
    "        n_heads: int\n",
    "                Number of attention heads.\n",
    "        mlp_ratio:float\n",
    "                Determines the hidden dimension of the MLP module.\n",
    "        qkv_bias:bool\n",
    "                If True then we include bias to the query,key and value projections.\n",
    "        p,attn_p:float\n",
    "                Dropout probability.\"\"\"\n",
    "    \n",
    "    \"\"\"Attributes\n",
    "        patch_embed: Patch Embed\n",
    "                Instance of Patch Embed layer.\n",
    "        cls_token:nn.Parameter\n",
    "                Learnable parameter that will represent the first token in the sequence\n",
    "                It has embed_dim elements.\n",
    "        pos_emb nn.Parameter\n",
    "                Positional embedding of the cls token+all the patches.\n",
    "                It has(n_patches+1)*embed_dim elements.\n",
    "        pos_drop nn.Dropout\n",
    "               Dropout layer.\n",
    "        blocks nn.ModuleList\n",
    "                List of Block modules.\n",
    "        norm:nn.LayerNorm\n",
    "                Layer normalization.\"\"\"\n",
    "    def __init__(self,\n",
    "                    img_size=384,\n",
    "                    patch_size=16,\n",
    "                    in_chans=3,\n",
    "                    n_classes=1000,\n",
    "                    embed_dim=768,\n",
    "                    depth=12,\n",
    "                    n_heads=12,\n",
    "                    mlp_ratio=4.,\n",
    "                    qkv_bias=True,\n",
    "                    p=0.,\n",
    "                    attn_p=0.,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.patch_embed=PatchEmbed(\n",
    "                            img_size=img_size,\n",
    "                            patch_size=patch_size,\n",
    "                            in_chans=in_chans,\n",
    "                            embed_dim=embed_dim\n",
    "                            )\n",
    "        self.cls_token=nn.Parameter(torch.zeros(1,1,embed_dim))\n",
    "        self.pos_embed=nn.Parameter(\n",
    "                        torch.zeros(1,1+self.patch_embed.n_patches,embed_dim)\n",
    "                        )\n",
    "        self.pos_drop=nn.Dropout(p=p)\n",
    "        self.blocks=nn.ModuleList(\n",
    "                    [\n",
    "                        Block(\n",
    "                            dim=embed_dim,\n",
    "                            n_heads=n_heads,\n",
    "                            mlp_ratio=mlp_ratio,\n",
    "                            qkv_bias=qkv_bias,\n",
    "                            p=p,\n",
    "                            attn_p=attn_p,\n",
    "                        )\n",
    "                        for _ in range(depth)\n",
    "                        \n",
    "                    ])\n",
    "        self.norm=nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.head=nn.Linear(embed_dim,n_classes)\n",
    "    def forward(self,x):\n",
    "        \"\"\"Run the forward pass.\n",
    "            Parameters\n",
    "            x: torch.Tensor\n",
    "                Shape(n_samples,in_chans,img_size,img_size)`.\n",
    "            Returns\n",
    "            logits: torch.Tensor\n",
    "                Logits over all the classes(n_samples,n_classes)`.\"\"\"\n",
    "                                               \n",
    "        n_samples= x.shape[0]\n",
    "        x=self.patch_embed(x)\n",
    "        \n",
    "        cls_token=self.cls_token.expand(\n",
    "                   n_samples,-1,-1 \n",
    "            ) # (n_samples,1,embed_dim)\n",
    "        x=torch.cat((cls_token,x),dim=1) # (n_samples, 1+n_patches, embed_dim)\n",
    "        x=x+self.pos_embed # (n_samples, 1+n_patches,embed_dim)\n",
    "        x=self.pos_drop(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ab21d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa7ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d5ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2461e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f120b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed6676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44678ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503588bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1577768d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f88396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda60de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9daee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd2810f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b904b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28e7edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a25e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90626d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
